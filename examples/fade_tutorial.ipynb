{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FADE Tutorial Notebook\n",
    "\n",
    "Welcome to the official tutorial for FADE â€” an open-source framework for evaluating how well internal features in transformer models align with their natural language descriptions.\n",
    "\n",
    "When working with modern language models, we often try to understand their internal mechanics by assigning natural language descriptions to specific neurons or features. But a critical question remains: **How accurately do these descriptions represent what the features actually encode?** FADE provides a systematic approach to answering this question.\n",
    "\n",
    "FADE introduces four complementary metrics that together provide a comprehensive assessment of feature-description alignment:\n",
    "\n",
    "- **Clarity**: Can we generate text that reliably activates a given feature? This tests whether the description is precise enough to be useful for creating activating content.\n",
    "- **Responsiveness**: Do examples that naturally activate the feature actually express the concept we've described? This tests whether the feature truly responds to instances of the concept in natural data.\n",
    "- **Purity**: Are the feature's activations specific to our described concept, or does it fire for unrelated content too? This tests whether the feature is dedicated to the concept or is polysemantic.\n",
    "- **Faithfulness**: Does modifying a feature's activation cause the model to generate more concept-related content? This tests the causal relationship between the feature and model outputs.\n",
    "\n",
    "By combining these perspectives, FADE helps identify mismatches between features and their descriptions, ultimately improving our understanding of what language models are actually learning.\n",
    "\n",
    "### ðŸ’¡ What This Notebook Covers\n",
    "- [Setup](#0-setup): Installing FADE and preparing your environment\n",
    "- [Quickstart](#1-quickstart): Learn how to run FADE on a single neuron with just a few lines of code\n",
    "- [Using Cached Activations](#2-using-cached-activations): Dramatically speed up evaluations by reusing precomputed activations\n",
    "- [Evaluating SAE features](#3-evaluating-sae-features): Apply FADE to Sparse Autoencoder features embedded in your model\n",
    "- [Running FADE with other Explainer Models](#4-running-fade-with-other-explainer-models): Use various LLMs as explainers beyond just OpenAI models\n",
    "- [Further Options](#5-further-options): Fine-tune FADE's behavior with advanced configuration options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving in, we need to set up our environment. FADE is designed to evaluate feature-concept alignment in transformer models, so we'll need to load a subject model, tokenizer, and a dataset to work with.\n",
    "\n",
    "First, let's install FADE. If you haven't done so already, FADE is available on PyPI:\n",
    "\n",
    "```bash\n",
    "pip install fade-language\n",
    "```\n",
    "\n",
    "Now, let's import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer # necessary for loading the subject model and tokenizer\n",
    "from datasets import load_dataset # necessary for loading the dataset\n",
    "from huggingface_hub import hf_hub_download # necessary for loading the SAE weights\n",
    "import numpy as np # necessary for loading SAE weights\n",
    "\n",
    "from fade import EvaluationPipeline # main entry point to the evaluation pipeline\n",
    "\n",
    "from utils import generate_activations # example function to generate activations \n",
    "from utils import SAEModuleWrapper # function to embed SAEs in the model as named modules\n",
    "from utils import JumpReLUSAEEncodeNoBOS, JumpReLUSAEDecode # custom modules to use a JumpReLU SAE as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FADE to effectively evaluate feature-concept alignment, we need a natural text dataset. Ideally, this dataset would closely match the distribution of the model's pre-training data to ensure valid results.\n",
    "\n",
    "For this tutorial, we'll use a small subset of the Neel Nanda Pile 10k dataset to keep things running quickly. For real-world evaluations, we strongly recommend:\n",
    "1. Using a larger, more representative dataset\n",
    "2. Pre-computing activations to speed up the process (covered in section 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load small example dataset\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\")\n",
    "dataset = {i: row[\"text\"][:250] for i, row in enumerate(dataset[\"train\"])}\n",
    "dataset = {i: row for i, row in dataset.items() if i < 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll demonstrate how to run a basic FADE evaluation on a single neuron. This is perfect for quickly testing the framework or exploring a specific feature of interest.\n",
    "\n",
    "For our example, we'll use Gemma 2 2B, a fairly compact but capable language model. Note that FADE works with virtually any transformer-based model available through HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our subject model - this is the model containing the features we want to evaluate\n",
    "subject_model_path = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(subject_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(subject_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the evaluation pipeline\n",
    "# We specify which LLM will be used as the \"evaluator\" for generating and rating content\n",
    "# This evaluator model helps assess how well features align with their descriptions\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai', # here we use OpenAI's API\n",
    "        'name': 'gpt-4o-mini-2024-07-18', # At the time of writing a strong but efficient model for evaluation\n",
    "        'api_key': 'YOUR-KEY-HERE', # Replace with your actual API key\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the evaluation pipeline with our model, tokenizer, and dataset\n",
    "eval_pipeline = EvaluationPipeline(\n",
    "    subject_model=model,  \n",
    "    subject_tokenizer=tokenizer, \n",
    "    dataset=dataset,\n",
    "    config=eval_config,\n",
    "    device=device, \n",
    "    verbose=True  # Set to True to see progress details\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which feature we want to evaluate\n",
    "neuron_module = \"model.layers[20].mlp.act_fn\"  # Path to the module containing our feature\n",
    "neuron_index = 42  # Index of the specific neuron/feature we're evaluating\n",
    "concept = \"The feature description you want to evaluate.\"  # What we think this feature represents\n",
    "\n",
    "# Run FADE evaluation to get our four alignment metrics\n",
    "(clarity, responsiveness, purity, faithfulness) = eval_pipeline.run(\n",
    "    neuron_module=neuron_module,\n",
    "    neuron_index=neuron_index,\n",
    "    concept=concept)\n",
    "\n",
    "print(f\"\\nClarity: {clarity} - How well we can generate text that activates this feature\", \n",
    "      f\"\\nResponsiveness: {responsiveness} - How well the feature responds to the concept in natural data\",\n",
    "      f\"\\nPurity: {purity} - How specific the feature is to our concept\",\n",
    "      f\"\\nFaithfulness: {faithfulness} - How much this feature causally influences generating concept-related content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Cached Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most significant performance optimizations in FADE is the ability to use cached activations. This approach delivers two major benefits:\n",
    "\n",
    "1. **Dramatic speed improvements**: Instead of recomputing activations for the entire dataset for each feature, you compute them once and reuse them.\n",
    "2. **Memory efficiency**: You can evaluate models on much larger datasets than would be possible with on-the-fly computation.\n",
    "\n",
    "This becomes especially critical when evaluating hundreds or thousands of features across a model, which is common in comprehensive interpretability studies.\n",
    "\n",
    "Let's see how to implement activation caching:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the subject model\n",
    "subject_model_path = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(subject_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(subject_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate activations for the entire dataset once\n",
    "# These will be reused for evaluating multiple features\n",
    "neuron_module = \"model.layers[20].mlp.act_fn\" \n",
    "activations = generate_activations(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    named_module=neuron_module,\n",
    "    device=device,\n",
    "    batch_size=4, # Adjust based on your available memory\n",
    ") # This returns raw activations for each sample in the dataset\n",
    "\n",
    "# Reduce the sequence dimension by taking the maximum activation for each feature\n",
    "# This gives us a single value per sample per neuron, which is what FADE expects\n",
    "activations = {i: torch.max(torch.abs(activation), dim=0)[0] for i, activation in activations.items()}\n",
    "\n",
    "# Create a function that returns the cached activations for a given neuron index\n",
    "# This function follows the ActivationLoader protocol expected by FADE\n",
    "def feature_activation_loader(neuron_index: int) -> torch.Tensor:\n",
    "    return torch.stack([activation[neuron_index] for i, activation in activations.items()])\n",
    "\n",
    "# Note: You can create and load the activations in any way you want, as long as you adhere to the ActivationLoader protocol in fade.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize custom configuration with OpenAI LLM\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai',\n",
    "        'name': 'gpt-4o-mini-2024-07-18',\n",
    "        'api_key': 'YOUR-KEY-HERE',\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_pipeline = EvaluationPipeline(\n",
    "    subject_model=model,  \n",
    "    subject_tokenizer=tokenizer, \n",
    "    dataset=dataset,\n",
    "    activations=feature_activation_loader, # we pass the function that returns the activations for a given neuron index\n",
    "    config=eval_config,\n",
    "    device=device, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we again evaluate a single neuron but this time we use the pre-computed activations\n",
    "neuron_module = \"model.layers[20].mlp.act_fn\"\n",
    "neuron_index = 42\n",
    "concept = \"The feature description you want to evaluate.\"\n",
    "\n",
    "(clarity, responsiveness, purity, faithfulness) = eval_pipeline.run(\n",
    "    neuron_module=neuron_module,\n",
    "    neuron_index=neuron_index,\n",
    "    concept=concept\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating SAE features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Autoencoders (SAEs) have become increasingly important in interpretability research because they can decompose the often polysemantic activations of regular model neurons into more monosemantic features. This decomposition can help make the features more interpretable, but how can we verify this?\n",
    "\n",
    "FADE is perfectly positioned to evaluate SAE features, letting us quantitatively assess whether SAE features actually align better with their descriptions than regular neurons. The only requirement is that the SAE must be embedded into the model's computational graph as a named module.\n",
    "\n",
    "Below, we'll demonstrate evaluating a JumpReLU SAE, a popular variant that has shown promising results in decomposing language model activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load our subject model of choice\n",
    "subject_model_path = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(subject_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(subject_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SAE parameters from a pretrained checkpoint\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v) for k, v in params.items()}\n",
    "\n",
    "# Separate encoder and decoder parameters\n",
    "pt_params_decode = {k: v for k, v in pt_params.items() if 'dec' in k}\n",
    "pt_params_encode = {k: v for k, v in pt_params.items() if k not in pt_params_decode}\n",
    "\n",
    "# Initialize the SAE encoder and decoder modules\n",
    "sae_encoder = JumpReLUSAEEncodeNoBOS(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae_encoder.load_state_dict(pt_params_encode)\n",
    "\n",
    "sae_decoder = JumpReLUSAEDecode(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae_decoder.load_state_dict(pt_params_decode)\n",
    "\n",
    "# Embed the SAE into the model's computational graph. This allows FADE to access the SAE's activations during evaluation as \"model.layers[20].additional_modules[0]\".\n",
    "model.model.layers[20] = SAEModuleWrapper(original_module=model.model.layers[20], additional_modules=[sae_encoder, sae_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize custom configuration with OpenAI LLM\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai',\n",
    "        'name': 'gpt-4o-mini-2024-07-18',\n",
    "        'api_key': 'YOUR-KEY-HERE',\n",
    "    },\n",
    "    \"subjectLLM\": {\n",
    "        \"sae_module\": True,  # This tells FADE to use SAE-type steering for the faithfulness metric\n",
    "    },\n",
    "}\n",
    "\n",
    "# initialize eval pipeline\n",
    "eval_pipeline = EvaluationPipeline(\n",
    "    subject_model=model,  \n",
    "    subject_tokenizer=tokenizer, \n",
    "    dataset=dataset,\n",
    "    config=eval_config,\n",
    "    device=device, \n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example neuron specification:\n",
    "neuron_module = \"model.layers[20].additional_modules[0]\"  # str of the added named SAE module!\n",
    "neuron_index = 42  # int of the neuron index in the named module\n",
    "concept = \"The feature description you want to evaluate.\"\n",
    "\n",
    "(clarity, responsiveness, purity, faithfulness) = eval_pipeline.run(\n",
    "    neuron_module=neuron_module,\n",
    "    neuron_index=neuron_index,\n",
    "    concept=concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running FADE with other Explainer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FADE is designed to be flexible in its choice of evaluation LLMs, supporting both proprietary models and locally hosted models through a variety of interfaces. This flexibility offers several important advantages:\n",
    "1. **Model selection flexibility**: Choose the model that best fits your specific evaluation needs - different models may excel at different types of concept understanding\n",
    "2. **Cost optimization**: Adjust your model choice based on budget constraints or evaluation scale\n",
    "3. **Research adaptability**: Some interpretability research may involve concepts that mainstream models are reluctant to evaluate due to safety filters\n",
    "4. **Bias mitigation**: Compare results across different models to identify and control for model-specific biases in evaluations\n",
    "5. **Privacy**: Keep sensitive data within your own infrastructure when needed\n",
    "\n",
    "FADE supports any OpenAI API-compatible model, whether hosted by OpenAI itself, Azure, or through local hosting like vLLM OpenAI servers. Additionally, it directly supports Ollama for straightforward local setups.\n",
    "\n",
    "Let's explore how to configure different types of evaluator models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load our subject model of choice\n",
    "subject_model_path = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(subject_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(subject_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration for different types of evaluation models\n",
    "# Simply comment out all options except the one you want to use\n",
    "\n",
    "# Option 1: OpenAI API (cloud-based)\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai',\n",
    "        'name': 'gpt-4o-mini-2024-07-18',\n",
    "        'api_key': 'YOUR-KEY-HERE',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Option 2: Ollama (local)\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'ollama',\n",
    "        'name': 'llama3.2',  # Must be installed in your Ollama instance\n",
    "        'base_url': 'http://127.0.0.1:11434', # Default Ollama port\n",
    "    }\n",
    "}\n",
    "\n",
    "# Option 3: vLLM Server (self-hosted API)\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai',\n",
    "        'name': 'llama3.2',\n",
    "        'api_key': 'optional_custom_key',\n",
    "        'base_url': 'http://example_server.com:8000', # url of your server\n",
    "    }\n",
    "}\n",
    "\n",
    "# Option 4: Azure OpenAI\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'azure',\n",
    "        'name': 'gpt-4o-mini-2024-07-18',\n",
    "        'api_key': 'YOUR-KEY-HERE',\n",
    "        'base_url': 'YOUR-AZURE-ENDPOINT-URL-HERE',\n",
    "        'api_version': '2024-02-01',\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_pipeline = EvaluationPipeline(\n",
    "    subject_model=model,  \n",
    "    subject_tokenizer=tokenizer, \n",
    "    dataset=dataset,\n",
    "    config=eval_config,\n",
    "    device=device, \n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_module = \"model.layers[20].mlp.act_fn\"\n",
    "neuron_index = 42\n",
    "concept = \"The feature description you want to evaluate.\"\n",
    "\n",
    "(clarity, responsiveness, purity, faithfulness) = eval_pipeline.run(\n",
    "    neuron_module=neuron_module,\n",
    "    neuron_index=neuron_index,\n",
    "    concept=concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Further Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FADE offers extensive customization options to fit different evaluation needs. Here, we'll highlight the most useful configuration groups and explain when you might want to adjust them. To see all options, please see the default_config.yaml file in the FADE repository. \n",
    "\n",
    "To use any of these options, simply add them to the configuration dictionary you've seen in previous examples:\n",
    "\n",
    "```python\n",
    "# Example of a configuration with custom settings\n",
    "eval_config = {\n",
    "    'evaluationLLM': {\n",
    "        'type': 'openai',\n",
    "        'name': 'gpt-4o-mini-2024-07-18',\n",
    "        'api_key': 'YOUR-KEY-HERE',\n",
    "    },\n",
    "    # Add custom experiment settings\n",
    "    'experiments': {\n",
    "        'gini_threshold': 0.6,  # Increase threshold for more selective faithfulness evaluation\n",
    "        'clarity': {\n",
    "            'llm_calls': 20,  # Generate more synthetic samples\n",
    "        },\n",
    "        'faithfulness': {\n",
    "            'modification_factors': [-100, -10, -1, 0, 1, 10, 100],  # Test wider range of modifications\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Then use this config as before\n",
    "eval_pipeline = EvaluationPipeline(\n",
    "    subject_model=model,  \n",
    "    subject_tokenizer=tokenizer, \n",
    "    dataset=dataset,\n",
    "    config=eval_config,\n",
    "    device=device, \n",
    "    verbose=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations specific to the subject model that is being investigated:\n",
    "```yaml\n",
    "subjectLLM:\n",
    "  sae_module: False # Whether the used module is a SAE module or not\n",
    "  batch_size: 4 # Batch size for the subject model\n",
    "````\n",
    "\n",
    "\n",
    "Configurations specific to the evaluation model that is being used to rate and generate samples:\n",
    "```yaml\n",
    "evaluationLLM:\n",
    "  type: \"type_of_evaluation_model\" # type of evaluation model as in: vllm, ollama, azure\n",
    "  name: \"modelname\" # name of the explainer model\n",
    "  api_key: \"Secret Key\" # API key if required\n",
    "  base_url: \"https://test.com/\" # base url where to reach the model\n",
    "  api_version: \"2024-02-02\" # API version of the model\n",
    "````\n",
    "\n",
    "\n",
    "Configurations specific to the actual evaluation. Here especially hyperparameters can be set for the different evaluation metrics. \n",
    "```yaml\n",
    "experiments:\n",
    "  rating_batch_size: 15 # Number of samples to be rated in one call to the evaluation model\n",
    "  gini_threshold: 0.5 # Threshold for confirming concept neuron and starting the faithfulness calculation. If Score is below this threshold, the concept neuron is not considered to be present. Should be between 0 and 1.\n",
    "  \n",
    "  clarity:\n",
    "    llm_calls: 15 # Number of parallel calls to the LLM given the prompt - determines the number of samples to be generated\n",
    "\n",
    "  responsiveness_and_purity:\n",
    "    num_samples: 500 # Number of samples to be rated in total - determines the number of calls to the explainer in combination with the explainer batch size\n",
    "    max_failed_retries: 1 # The maximum number of retries when samples fail to generate ratings.\n",
    "    max_sparse_retries: 1 # The maximum number of retries if not enough samples have non-zero ratings.\n",
    "    retry_sparse_threshold: 15 # The number of non-zero ratings below which additional samples are generated.\n",
    "    repeat_non_zeros: 0 # The number of times to repeat ratings for samples with non-zero ratings.\n",
    "    rating_top_sampling_percentage: 0.1 # Percentage of samples taken from the top of the activations\n",
    "\n",
    "  faithfulness:\n",
    "    modification_factors: [-50, -10, -1, 0, 1, 10, 50] # Modification factors to be applied to the neuron\n",
    "    num_samples: 50 # Number of samples to be generated for each modification factor - determines the number of calls to the explainer in combination with the explainer batch size\n",
    "    generation_length: 30 # Token length of the generated samples\n",
    "    max_failed_retries: 1 # The maximum number of retries when samples fail to generate ratings.\n",
    "    max_sparse_retries: 0 # The maximum number of retries if not enough samples have non-zero ratings.\n",
    "    retry_sparse_threshold: 20 # The number of non-zero ratings below which additional samples are generated.\n",
    "    repeat_non_zeros: 0 # The number of times to repeat ratings for samples with non-zero ratings.\n",
    "    rating_top_sampling_percentage: 0.1 # Percentage of samples taken from the top of the activations\n",
    "````\n",
    "\n",
    "Configurations regarding the prompts\n",
    "```yaml\n",
    "prompts:\n",
    "  generation: \"...\" # The prompt used to generate the samples\n",
    "  rating: \"...\" # The prompt used to rate the samples\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
